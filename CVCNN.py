# -*- coding: utf-8 -*-
"""CVCNN_sample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zTiixRxkxGEjhTFb0VeoiH3-oYjixmxm
"""

# requirements pip install torch numpy scikit-learn scipy pywt
# Imports
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from scipy.signal import hilbert, stft, find_peaks
import pywt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report
)

from torch.utils.data import Dataset, DataLoader

# This is for example. Use your own data
def generate_ecg(length=3600, fs=360):
    t = np.linspace(0, length/fs, length)
    ecg = (
        1.2 * np.sin(2*np.pi*1.2*t) +
        0.25 * np.sin(2*np.pi*2.5*t) +
        0.1 * np.random.randn(length)
    )
    return ecg

from scipy.signal import hilbert, stft
import numpy as np

def process_beat(beat, fs=360, nperseg=64, noverlap=32):
    """
    Convert a single ECG beat to a complex time-frequency representation.

    Parameters
    ----------
    beat : 1D numpy array
        ECG beat (real-valued)
    fs : int
        Sampling frequency
    nperseg : int
        STFT window length
    noverlap : int
        STFT overlap

    Returns
    -------
    Zxx : 2D complex numpy array
        STFT of analytic ECG beat
    """

    # 1. Hilbert transform → analytic signal
    analytic = hilbert(beat)

    # 2. Short-Time Fourier Transform
    _, _, Zxx = stft(
        analytic,
        fs=fs,
        nperseg=nperseg,
        noverlap=noverlap,
        boundary=None
    )

    return Zxx.astype(np.complex64)

# Signal Preprocessing
def standard_scale(signal):
    scaler = StandardScaler()
    return scaler.fit_transform(signal.reshape(-1, 1)).flatten()

def wavelet_denoise(signal, wavelet="db6", level=5):
    coeffs = pywt.wavedec(signal, wavelet, level=level)
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(len(signal)))

    coeffs[1:] = [pywt.threshold(c, value=uthresh, mode="soft")
                  for c in coeffs[1:]]
    return pywt.waverec(coeffs, wavelet)

def r_peak_detect(ecg, fs=360):
    distance = int(0.25 * fs)  # MUST be int
    distance = max(distance, 1)
    peaks, _ = find_peaks(ecg, distance=distance, height=np.std(ecg))
    return peaks

def analytic_signal(ecg):
    return hilbert(ecg)

def compute_stft(signal, fs=360):
    _, _, Zxx = stft(signal, fs=fs, nperseg=128)
    return Zxx

def preprocess_ecg(ecg, label, fs=360):
    ecg = standard_scale(ecg)
    ecg = wavelet_denoise(ecg)

    r_peaks = r_peak_detect(ecg, fs)

    X, y = [], []

    # SAFETY CHECK
    if len(r_peaks) == 0:
        return X, y

    for r in r_peaks:
        if r-90 >= 0 and r+110 < len(ecg):
            beat = ecg[r-90:r+110]
            tf = process_beat(beat)
            X.append(tf)
            y.append(label)

    return X, y

X_all, y_all = [], []

for label in range(3):
    for _ in range(15):
        ecg = generate_ecg()
        X, y = preprocess_ecg(ecg, label)

        if len(X) == 0:
            continue  # skip bad samples

        X_all.extend(X)
        y_all.extend(y)


X_all = np.array(X_all)
y_all = np.array(y_all)

# reshape for CNN: (N, C, H, W)
X_all = X_all[:, np.newaxis, :, :]

X_all = np.array(X_all)
y_all = np.array(y_all)

print("Total samples:", len(y_all))
print("Class distribution:", np.bincount(y_all))

X_train, X_temp, y_train, y_temp = train_test_split(
    X_all, y_all, test_size=0.2, stratify=y_all
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, stratify=y_temp
)

class ECGDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.complex64)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_loader = DataLoader(ECGDataset(X_train, y_train), batch_size=32, shuffle=True)
test_loader  = DataLoader(ECGDataset(X_test, y_test), batch_size=32)

class ComplexConv2d(nn.Module):
    def __init__(self, in_c, out_c, k, padding=1):
        super().__init__()
        self.real = nn.Conv2d(in_c, out_c, k, padding=padding)
        self.imag = nn.Conv2d(in_c, out_c, k, padding=padding)

    def forward(self, x):
        real = self.real(x.real) - self.imag(x.imag)
        imag = self.real(x.imag) + self.imag(x.real)
        return torch.complex(real, imag)

class ComplexReLU(nn.Module):
    def forward(self, x):
        return torch.complex(F.relu(x.real), F.relu(x.imag))

class ComplexMaxPool2d(nn.Module):
    def __init__(self, k):
        super().__init__()
        self.pool = nn.MaxPool2d(k)

    def forward(self, x):
        return torch.complex(self.pool(x.real), self.pool(x.imag))


class ComplexLinear(nn.Module):
    def __init__(self, in_f, out_f):
        super().__init__()
        self.real = nn.Linear(in_f, out_f)
        self.imag = nn.Linear(in_f, out_f)

    def forward(self, x):
        real = self.real(x.real) - self.imag(x.imag)
        imag = self.real(x.imag) + self.imag(x.real)
        return torch.complex(real, imag)

class CVCNN(nn.Module):
    def __init__(self, n_classes):
        super().__init__()

        self.block1 = nn.Sequential(
            ComplexConv2d(1, 16, 3, padding=1),
            ComplexReLU(),
            ComplexMaxPool2d(2),
            ComplexDropout(0.25)
        )

        self.block2 = nn.Sequential(
            ComplexConv2d(16, 32, 3, padding=1),
            ComplexReLU(),
            ComplexMaxPool2d(2),
            ComplexDropout(0.25)
        )

        # We'll infer this dynamically (see below)
        self.fc1 = None
        self.fc2 = None
        self.n_classes = n_classes

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)

        x = x.view(x.size(0), -1)

        # Lazy initialization (VERY IMPORTANT)
        if self.fc1 is None:
            self.fc1 = ComplexLinear(x.shape[1], 64).to(x.device)
            self.fc2 = ComplexLinear(64, self.n_classes).to(x.device)

        x = self.fc1(x)
        x = self.fc2(x)
        return x

class ComplexDropout(nn.Module):
    def __init__(self, p=0.5):
        super().__init__()
        self.p = p

    def forward(self, x):
        if not self.training or self.p == 0.0:
            return x

        mask = (torch.rand_like(x.real) > self.p).float()
        mask = mask / (1.0 - self.p)

        return torch.complex(x.real * mask, x.imag * mask)

class ComplexCrossEntropy(nn.Module):
    def forward(self, logits, target):
        return F.cross_entropy(torch.abs(logits), target)

model = CVCNN(n_classes=3)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

# Train
for epoch in range(5):
    model.train()

    running_loss = 0.0
    correct = 0
    total = 0

    for x, y in train_loader:
        optimizer.zero_grad()

        logits = model(x)
        probs = torch.abs(logits)              # magnitude for classification
        loss = criterion(probs, y)

        loss.backward()
        optimizer.step()

        # ----- accumulate stats -----
        running_loss += loss.item() * y.size(0)

        preds = torch.argmax(probs, dim=1)
        correct += (preds == y).sum().item()
        total += y.size(0)

    epoch_loss = running_loss / total
    epoch_acc = correct / total

    print(
        f"Epoch [{epoch+1}/5] "
        f"Train Loss: {epoch_loss:.4f} "
        f"Train Acc: {epoch_acc:.4f}"
    )

model.eval()
y_true, y_pred = [], []

with torch.no_grad():
    for x, y in test_loader:
        logits = model(x)

        # ✅ convert complex → real
        scores = torch.abs(logits)

        preds = torch.argmax(scores, dim=1)

        y_true.extend(y.numpy())
        y_pred.extend(preds.numpy())


print("Accuracy :", accuracy_score(y_true, y_pred))
print("Precision:", precision_score(y_true, y_pred, average="weighted", zero_division=0))
print("Recall   :", recall_score(y_true, y_pred, average="weighted", zero_division=0,))
print("F1-score :", f1_score(y_true, y_pred, average="weighted", zero_division=0,))

print("\nConfusion Matrix:")
print(confusion_matrix(y_true, y_pred))

print("\nClassification Report:")
print(classification_report(y_true, y_pred, digits=4))